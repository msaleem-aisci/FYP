{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhhL2w/wyFeqtw/x+oiz1b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msaleem-aisci/FYP/blob/main/LSTM_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Vfl33kzC3l31"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tanh:\n",
        "    def forward(self, z):\n",
        "        return np.tanh(z)\n",
        "\n",
        "    def backward(self, delta, z):\n",
        "        return delta * (1 - np.tanh(z)**2)\n",
        "\n",
        "class Sigmoid:\n",
        "    def forward(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def backward(self, delta, z):\n",
        "        sig = self.forward(z)\n",
        "        return delta * (sig * (1 - sig))\n",
        "\n",
        "class Softmax:\n",
        "    def forward(self, z):\n",
        "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
        "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
        "\n",
        "    def backward(self, delta, z):\n",
        "        return delta"
      ],
      "metadata": {
        "id": "hg0bI40TBSsT"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense:\n",
        "    def __init__(self, units, input_dim):\n",
        "        self.weights = np.random.randn(units, input_dim) * 0.01\n",
        "        self.bias = np.zeros((units, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.inputs = x\n",
        "        self.z = np.dot(self.weights, x) + self.bias\n",
        "\n",
        "        exp_z = np.exp(self.z - np.max(self.z))\n",
        "        self.probs = exp_z / np.sum(exp_z)\n",
        "        return self.probs\n",
        "\n",
        "    def backward(self, target_idx, lr):\n",
        "        dy = self.probs.copy()\n",
        "        dy[target_idx] -= 1\n",
        "\n",
        "        dW = np.dot(dy, self.inputs.T)\n",
        "        db = dy\n",
        "        dX = np.dot(self.weights.T, dy)\n",
        "\n",
        "        self.weights -= lr * dW\n",
        "        self.bias -= lr * db\n",
        "        return dX"
      ],
      "metadata": {
        "id": "NygKop6W_rgA"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM:\n",
        "    def __init__(self, units, input_dim):\n",
        "        self.units = units\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        def init(r, c): return np.random.randn(r, c) * np.sqrt(2.0/(r+c))\n",
        "\n",
        "        self.params = {\n",
        "            'Wf': init(units, input_dim), 'Uf': init(units, units), 'bf': np.zeros((units, 1)),\n",
        "            'Wi': init(units, input_dim), 'Ui': init(units, units), 'bi': np.zeros((units, 1)),\n",
        "            'Wc': init(units, input_dim), 'Uc': init(units, units), 'bc': np.zeros((units, 1)),\n",
        "            'Wo': init(units, input_dim), 'Uo': init(units, units), 'bo': np.zeros((units, 1))\n",
        "        }\n",
        "        self.cache = []\n",
        "\n",
        "    def sigmoid(self, x): return 1 / (1 + np.exp(-x))\n",
        "    def tanh(self, x): return np.tanh(x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.h = np.zeros((self.units, 1))\n",
        "        self.c = np.zeros((self.units, 1))\n",
        "        self.cache = []\n",
        "\n",
        "        for x_t in X:\n",
        "            x_t = x_t.reshape(-1, 1)\n",
        "            p = self.params\n",
        "\n",
        "            f = self.sigmoid(np.dot(p['Wf'], x_t) + np.dot(p['Uf'], self.h) + p['bf'])\n",
        "            i = self.sigmoid(np.dot(p['Wi'], x_t) + np.dot(p['Ui'], self.h) + p['bi'])\n",
        "            c_tilde = self.tanh(np.dot(p['Wc'], x_t) + np.dot(p['Uc'], self.h) + p['bc'])\n",
        "\n",
        "            self.c = (f * self.c) + (i * c_tilde)\n",
        "            o = self.sigmoid(np.dot(p['Wo'], x_t) + np.dot(p['Uo'], self.h) + p['bo'])\n",
        "            self.h = o * self.tanh(self.c)\n",
        "\n",
        "            self.cache.append({'x':x_t, 'h_prev': self.cache[-1]['h'] if self.cache else np.zeros_like(self.h),\n",
        "                               'c_prev': self.cache[-1]['c'] if self.cache else np.zeros_like(self.c),\n",
        "                               'f':f, 'i':i, 'c_tilde':c_tilde, 'o':o, 'c':self.c, 'h':self.h})\n",
        "        return self.h\n",
        "\n",
        "    def backward(self, dh, lr):\n",
        "        grads = {k: np.zeros_like(v) for k, v in self.params.items()}\n",
        "        dh_next = np.zeros_like(self.h)\n",
        "        dc_next = np.zeros_like(self.c)\n",
        "\n",
        "        for step in reversed(self.cache):\n",
        "            dh_total = dh + dh_next\n",
        "            tanh_c = self.tanh(step['c'])\n",
        "\n",
        "            do = dh_total * tanh_c\n",
        "            do_raw = do * step['o'] * (1 - step['o'])\n",
        "            grads['Wo'] += np.dot(do_raw, step['x'].T); grads['Uo'] += np.dot(do_raw, step['h_prev'].T); grads['bo'] += do_raw\n",
        "\n",
        "            dc = (dh_total * step['o'] * (1 - tanh_c**2)) + dc_next\n",
        "\n",
        "            dc_tilde = dc * step['i']\n",
        "            dc_tilde_raw = dc_tilde * (1 - step['c_tilde']**2)\n",
        "            grads['Wc'] += np.dot(dc_tilde_raw, step['x'].T); grads['Uc'] += np.dot(dc_tilde_raw, step['h_prev'].T); grads['bc'] += dc_tilde_raw\n",
        "\n",
        "            di = dc * step['c_tilde']\n",
        "            di_raw = di * step['i'] * (1 - step['i'])\n",
        "            grads['Wi'] += np.dot(di_raw, step['x'].T); grads['Ui'] += np.dot(di_raw, step['h_prev'].T); grads['bi'] += di_raw\n",
        "\n",
        "            df = dc * step['c_prev']\n",
        "            df_raw = df * step['f'] * (1 - step['f'])\n",
        "            grads['Wf'] += np.dot(df_raw, step['x'].T); grads['Uf'] += np.dot(df_raw, step['h_prev'].T); grads['bf'] += df_raw\n",
        "\n",
        "            dc_next = dc * step['f']\n",
        "            dh_next = (np.dot(self.params['Uo'].T, do_raw) + np.dot(self.params['Uc'].T, dc_tilde_raw) +\n",
        "                       np.dot(self.params['Ui'].T, di_raw) + np.dot(self.params['Uf'].T, df_raw))\n",
        "\n",
        "        for k in self.params:\n",
        "            np.clip(grads[k], -1, 1, out=grads[k])\n",
        "            self.params[k] -= lr * grads[k]\n",
        "        return dh_next"
      ],
      "metadata": {
        "id": "UvwPFrNb3rQU"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextProcessor:\n",
        "    def __init__(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        self.words = text.split()\n",
        "\n",
        "        self.vocab = sorted(list(set(self.words)))\n",
        "        self.word_to_ix = { w:i for i,w in enumerate(self.vocab) }\n",
        "        self.ix_to_word = { i:w for i,w in enumerate(self.vocab) }\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "\n",
        "    def create_sequences(self, seq_length):\n",
        "        X, y = [], []\n",
        "        for i in range(len(self.words) - seq_length):\n",
        "            seq_in = self.words[i : i + seq_length]\n",
        "            seq_out = self.words[i + seq_length]\n",
        "\n",
        "            X.append([self.word_to_ix[w] for w in seq_in])\n",
        "            y.append(self.word_to_ix[seq_out])\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def one_hot(self, idx):\n",
        "        vec = np.zeros((self.vocab_size, 1))\n",
        "        vec[idx] = 1\n",
        "        return vec"
      ],
      "metadata": {
        "id": "3-T4VunBEIiU"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = \"\"\"\n",
        "Deep learning is a subset of machine learning based on artificial neural networks.\n",
        "We implemented an LSTM from scratch to understand how recurrent neural networks work.\n",
        "The vanishing gradient problem was solved by the LSTM architecture using gates.\n",
        "We are now building a next word predictor to test our understanding.\n",
        "Machine learning requires data and algorithms to make predictions.\n",
        "Neural networks are inspired by the human brain and its neurons.\n",
        "Gradients flow backward to update the weights of the model.\n",
        "The forget gate decides what information to throw away from the cell state.\n",
        "The input gate decides what new information to store in the cell state.\n",
        "The output gate decides what the next hidden state should be.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_A68_evr3rKn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH = 3\n",
        "HIDDEN_SIZE = 32\n",
        "LEARNING_RATE = 0.01\n",
        "EPOCHS = 300"
      ],
      "metadata": {
        "id": "AtJNOuzHAOie"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = TextProcessor(raw_text)\n",
        "X_data, y_data = processor.create_sequences(SEQ_LENGTH)\n",
        "\n",
        "lstm = LSTM(units=HIDDEN_SIZE, input_dim=processor.vocab_size)\n",
        "dense = Dense(units=processor.vocab_size, input_dim=HIDDEN_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPdun0dvCRB-",
        "outputId": "ed2bc74b-d2e8-4372-e848-2c7d1e74ea24"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Stats: 117 words, 76 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AefnmDUTFzLO",
        "outputId": "ad5d3015-a501-42eb-fcc9-b6b16efc655d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "76"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Training on {len(X_data)} sequences...\")\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(len(X_data)):\n",
        "        inputs = [processor.one_hot(idx) for idx in X_data[i]]\n",
        "        target_idx = y_data[i]\n",
        "\n",
        "        h_last = lstm.forward(inputs)\n",
        "        probs = dense.forward(h_last)\n",
        "\n",
        "        loss = -np.log(probs[target_idx, 0] + 1e-9)\n",
        "        total_loss += loss\n",
        "\n",
        "        dh = dense.backward(target_idx, LEARNING_RATE)\n",
        "        lstm.backward(dh, LEARNING_RATE)\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}, Avg Loss: {total_loss / len(X_data):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Kn8tQtCCUVK",
        "outputId": "b0785131-33d1-4883-938f-7566bfdfd4d6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 114 sequences...\n",
            "Epoch 0, Avg Loss: 4.3246\n",
            "Epoch 50, Avg Loss: 0.6893\n",
            "Epoch 100, Avg Loss: 0.1080\n",
            "Epoch 150, Avg Loss: 0.0749\n",
            "Epoch 200, Avg Loss: 0.0664\n",
            "Epoch 250, Avg Loss: 0.0620\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_words(start_text, n_words=1):\n",
        "\n",
        "    clean_text = start_text.lower()\n",
        "    clean_text = re.sub(r'[^\\w\\s]', '', clean_text)\n",
        "    words = clean_text.split()\n",
        "\n",
        "    current_words = words[-SEQ_LENGTH:]\n",
        "\n",
        "    generated_output = []\n",
        "\n",
        "    print(f\"\\n--- Predicting next {n_words} words for: '{start_text}' ---\")\n",
        "\n",
        "    for _ in range(n_words):\n",
        "        try:\n",
        "            inputs = [processor.one_hot(processor.word_to_ix[w]) for w in current_words]\n",
        "        except KeyError:\n",
        "            print(\"Error: Word not in vocabulary.\")\n",
        "            return\n",
        "\n",
        "        h_last = lstm.forward(inputs)\n",
        "        probs = dense.forward(h_last)\n",
        "\n",
        "        pred_idx = np.argmax(probs)\n",
        "        pred_word = processor.ix_to_word[pred_idx]\n",
        "\n",
        "        generated_output.append(pred_word)\n",
        "\n",
        "        current_words.append(pred_word)\n",
        "        current_words.pop(0)\n",
        "\n",
        "    print(f\"Prediction: {' '.join(generated_output)}\")"
      ],
      "metadata": {
        "id": "t3z8WzGBCc4s"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_next_words(\"Machine Learning\", n_words=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTiDu426E30r",
        "outputId": "a15b2d65-49de-4ac7-a72b-a8f51ed91881"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Predicting next 5 words for: 'Machine Learning' ---\n",
            "Prediction: learning update update update update\n"
          ]
        }
      ]
    }
  ]
}